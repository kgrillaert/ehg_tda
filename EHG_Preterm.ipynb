{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "222a2a04",
   "metadata": {},
   "source": [
    "# Preterm Birth Prediction Using EHG\n",
    "\n",
    "This project is using topological data analysis of electrohysterograph samples to predict preterm births. TRhe goal is to determine if TDA can enhance the classification of signals and subsequent prediction of preterm birth. \n",
    "\n",
    "Previous literature: use of persistent homology to classify electromyography signals (hand movements) https://www.proquest.com/openview/8312b24e2d9eb9f154c172d4705d4409/1?pq-origsite=gscholar&cbl=5444811"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5c105b",
   "metadata": {},
   "source": [
    "## Data Access  \n",
    "\n",
    "https://physionet.org/content/tpehgdb/1.0.1/tpehgdb/\n",
    "\n",
    "Goldberger, A., Amaral, L., Glass, L., Hausdorff, J., Ivanov, P. C., Mark, R., ... & Stanley, H. E. (2000). PhysioBank, PhysioToolkit, and PhysioNet: Components of a new research resource for complex physiologic signals. Circulation [Online]. 101 (23), pp. e215–e220.\n",
    "\n",
    "Gašper Fele-Žorž, Gorazd Kavšek, Živa Novak-Antolič and Franc Jager. A comparison of various linear and non-linear signal processing techniques to separate uterine EMG records of term and pre-term delivery groups. Medical & Biological Engineering & Computing, 46(9):911-922 (2008)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aced3a52",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "\n",
    "\"The Term-Preterm EHG Database, a collection of electrohysterogram (EHG: uterine EMG) recordings obtained at the University Medical Centre Ljubljana from 300 pregnant women, has been contributed to PhysioNet. The collection includes recordings from 262 women who had full-term pregnancies and 38 whose pregnancies ended prematurely; 162 of the recordings were made before the 26th week of gestation, and 138 later.\"\n",
    "\n",
    "Each record is composed of three channels, recorded from 4 electrodes (E1, E2, E3, E4).  \n",
    "The differences in the electrical potentials of the electrodes were recorded, producing 3 channels:  \n",
    " - S1 = E2–E1 (first channel)\n",
    " - S2 = E2–E3 (second channel)  \n",
    " - S3 = E4–E3 (third channel)  \n",
    "\n",
    "The individual records are 30 minutes in duration. Each signal has been digitized at 20 samples per second per channel with 16-bit resolution over a range of ±2.5 millivolts.  \n",
    "\n",
    "Each signal was digitally filtered using 3 different 4-pole digital Butterworth filters with a double-pass filtering scheme. The band-pass cut-off frequencies were:  \n",
    "\n",
    " - 0.08Hz to 4Hz\n",
    " - 0.3Hz to 3Hz\n",
    " - 0.3Hz to 4Hz\n",
    "\n",
    "The records in the database contain both the original and filtered signals. The records are in WFDB format. Each record consists of two files, a header file (.hea) containing information regarding the record and the data file (.dat) containing signal data.\n",
    "\n",
    "An accompanying file (tpehgdb.smr) summarizes clinical information of each record, describing whether the corresponding pregnancy ended on term (> 37 weeks) or prematurely (≤ 37 weeks), and whether the record was obtained before the 26th week of gestation or during or after the 26th week of gestation.  \n",
    "\n",
    "Visualize the waveform: https://physionet.org/lightwave/?db=tpehgdb/1.0.1  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c5ba20",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Virtual Environment \n",
    "my virtual environment name: ehg_preterm   \n",
    "Best practices is to activate the virtual environment from CLI (see git bash example below)  \n",
    "Navigate to project directory  \n",
    " source ./ehg_preterm/Scripts/activate  \n",
    " deactivate  \n",
    "\n",
    "## Requirements\n",
    "jupyter  \n",
    "pandas  \n",
    "numpy  \n",
    "scikit-learn  \n",
    "matplotlib  \n",
    "seaborn  \n",
    "wfdb #for waveform data  \n",
    "tensorflow  \n",
    "glob  \n",
    "ripser \n",
    "\n",
    "### ripser requirements for Windows\n",
    "- MinGW-w64 for Windows https://winlibs.com/ (also make sure to update system Path)\n",
    "- Microsoft C++ Build Tools \"Desktop development with C++\" workload https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
    "- install cython before installing ripser\n",
    "\n",
    "\n",
    "### wfdb readthedocs\n",
    "https://wfdb.readthedocs.io/en/latest/wfdb.html\n",
    "\n",
    "### smr filetype\n",
    "This filetype is proprietary to Spike2 by Cambridge Electronic Design. I did not have luck parsing it with python, so I used https://filext.com/file-extension/SMR, pasted into a txt file and read it to csv. This would not be ideal with larger files.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d34074b-b0f4-48a0-b744-7b0caeca26c5",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9430c69d-e7e1-4ead-8cf5-11705438089a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib\n",
    "import seaborn\n",
    "import wfdb\n",
    "import os\n",
    "import glob\n",
    "import cython\n",
    "import ripser\n",
    "from ripser import ripser\n",
    "import matplotlib.pyplot as plt\n",
    "from persim import plot_diagrams\n",
    "from ripser import Rips\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9e082e-6059-4f82-bc45-103aac478b93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"testing the kernel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c7a01a-6958-453a-b420-bbb3008fb002",
   "metadata": {},
   "source": [
    "# Inspect and Import Data \n",
    ".dat and .hea files from database tpehgdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b397ac9",
   "metadata": {},
   "source": [
    "## Visualize one patient record\n",
    "\n",
    "Thank you to ProtoBioengineering for the tutorial: https://medium.com/@protobioengineering/how-to-get-heart-data-from-the-mit-bih-arrhythmia-database-e452d4bf7215 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dac811",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "patient_record = wfdb.rdrecord(\"tpehgdb/tpehg1007\")\n",
    "wfdb.plot_wfdb(patient_record) # Visualiztion of 12 channels (sensors/leads) for the patient record \n",
    "print(patient_record.__dict__) # Dictionary of metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea685714",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Per wfdb dot notation, metadata can be accessed via patient_record.XYZ\n",
    "patient_number = patient_record.record_name\n",
    "leads = patient_record.sig_name # Names of leads and filtered leads \n",
    "comments = patient_record.comments\n",
    "sig_len = patient_record.sig_len # Number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3f0035",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(patient_number)\n",
    "print(leads)\n",
    "print(comments)\n",
    "print(sig_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d4fa10",
   "metadata": {},
   "source": [
    "## Extract data to csv\n",
    "Write loop that extracts relevant information; each patient to one csv. \n",
    "Thank you to Abhishek Patil: https://github.com/abhilampard/Physionet-CSV-Conversion/blob/master/script.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed72d661",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check current working directory\n",
    "os.getcwd()\n",
    "\n",
    "#Change working directory if necessary\n",
    "os.chdir('/home/katie_grillaert/ehg_preterm/tpehgdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298b6071",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code to convert all .dat files (ECG signals) in a folder to CSV format \n",
    "# @author: Abhishek Patil\n",
    "\n",
    "# THIS CODE IMPORTS 12 cols, one for each lead, with the first line as the header. \n",
    "#dat_files = glob.glob('*.dat') #Get list of all .dat files in the current folder\n",
    "#df=pd.DataFrame(data=dat_files)\n",
    "#df.to_csv(\"files_list.csv\",index=False,header=None) #Write the list to a CSV file\n",
    "#files=pd.read_csv(\"files_list.csv\",header=None)\n",
    "\n",
    "#Given the master csv, write the data to a csv for each patient:\n",
    "\n",
    "dat_files = glob.glob('*.dat')  # Get list of all .dat files in the current folder\n",
    "\n",
    "df = pd.DataFrame(data=dat_files)\n",
    "df.to_csv(\"/home/katie_grillaert/ehg_preterm/files_list.csv\", index=False, header=None)  # Write the list to a CSV file\n",
    "files = pd.read_csv(\"/home/katie_grillaert/ehg_preterm/files_list.csv\", header=None)\n",
    "output_directory = \"/home/katie_grillaert/ehg_preterm\" \n",
    "\n",
    "for i in range(1, len(files) + 1):\n",
    "    recordname = str(files.iloc[[i]])\n",
    "    \n",
    "    # Replace multiple consecutive spaces with a single space\n",
    "    recordname = ' '.join(recordname.split())\n",
    "\n",
    "    # Remove leading/trailing spaces\n",
    "    recordname = recordname.strip()\n",
    "\n",
    "    # Replace newline characters in the file name\n",
    "    recordname = recordname.replace('\\n', '')\n",
    "\n",
    "    # Initialize the recordname_new variable\n",
    "    recordname_new = \"\"\n",
    "\n",
    "    try:\n",
    "        # Try with original extraction\n",
    "        recordname_new = recordname[-13:-4]\n",
    "        record = wfdb.rdsamp(recordname_new)  # rdsamp() returns the signal as a numpy array\n",
    "    except FileNotFoundError:\n",
    "        try:\n",
    "            # Retry with alternative extraction\n",
    "            recordname_new = recordname[-12:-4]\n",
    "            record = wfdb.rdsamp(recordname_new)  # rdsamp() returns the signal as a numpy array\n",
    "        except FileNotFoundError:\n",
    "            # If both attempts fail, skip this file\n",
    "            print(f\"File not found: {recordname}\")\n",
    "            continue\n",
    "\n",
    "    record = np.asarray(record[0])\n",
    "\n",
    "    path = recordname_new + \".csv\"\n",
    "    np.savetxt(path, record, delimiter=\",\")  # Writing the CSV for each record\n",
    "    print(\"Files done: %s/%s\" % (i, len(files)))\n",
    "\n",
    "print(\"\\nAll files done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f00210b-023f-41ee-91f7-24bcaedfde4b",
   "metadata": {},
   "source": [
    "### Test: Look at one csv example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180bfb09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/katie_grillaert/ehg_preterm/tpehgdb/tpehg1021.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d602bb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbaea67",
   "metadata": {},
   "source": [
    "# Analyse persistent homology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d406295",
   "metadata": {},
   "source": [
    "## Filtration information\n",
    "\n",
    "maxdim (int, optional, default 1) – Maximum homology dimension computed. Will compute all dimensions lower than and equal to this value. For 1, H_0 and H_1 will be computed.  \n",
    "thresh (float, default infinity) – Maximum distances considered when constructing filtration. If infinity, compute the entire filtration.  \n",
    "coeff (int prime, default 2) – Compute homology with coefficients in the prime field Z/pZ for p=coeff.  \n",
    "do_cocycles (bool) – Indicator of whether to compute cocycles, if so, we compute and store cocycles in the cocycles_ dictionary Rips member variable  \n",
    "n_perm (int) – The number of points to subsample in a “greedy permutation,” or a furthest point sampling of the points. These points will be used in lieu of the full point cloud for a faster computation, at the expense of some accuracy, \n",
    "which can be bounded as a maximum bottleneck distance to all diagrams on the original point set  \n",
    "verbose (boolean) – Whether to print out information about this object as it is constructed  \n",
    "\n",
    "https://ripser.scikit-tda.org/en/latest/reference/stubs/ripser.Rips.html   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63658518-3177-41d3-b877-6369b2cabec6",
   "metadata": {},
   "source": [
    "## Explore one observation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c1286e-8fbc-424f-87a4-a0f17e75058f",
   "metadata": {},
   "source": [
    "### Visualize persistence diagram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f72be6e-c55f-4ad1-9b68-594a63cd8758",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#So much data... will need to research proper way to filter and create sparse matrix, and probably sample on top of that. For now, a simple sample. \n",
    "unfiltered_sensors = \n",
    "\n",
    "sample_fraction = 0.01\n",
    "df_sample = df.sample(frac=sample_fraction, random_state=42) \n",
    "print(\"sample prepared\")\n",
    "\n",
    "result = ripser(df_sample, maxdim=2, coeff=2)\n",
    "print(\"result calculated\")\n",
    "\n",
    "# Plot the persistent homology diagram\n",
    "plt.figure(figsize=(10, 5))\n",
    "diagrams = (result)['dgms']\n",
    "plot_diagrams(diagrams, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea3da4d-bd38-4f03-ae30-59f5c294d5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.astype(str)\n",
    "\n",
    "new_column_names = {'0': 's1_unfilt', \n",
    "                    '1': 's1_filter_a', \n",
    "                    '2': 's1_filter_b',\n",
    "                    '3': 's1_filter_c',\n",
    "                    '4': 's2_unfilt', \n",
    "                    '5': 's2_filter_a', \n",
    "                    '6': 's2_filter_b',\n",
    "                    '7': 's2_filter_c',\n",
    "                    '8': 's3_unfilt', \n",
    "                    '9': 's3_filter_a', \n",
    "                    '10': 's3_filter_b',\n",
    "                    '11': 's3_filter_c'}\n",
    "\n",
    "df.rename(columns=new_column_names, inplace=True)\n",
    "\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58fccf0-613e-40f7-ae77-c1b7da17a480",
   "metadata": {},
   "source": [
    "### Examine dictionary of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182e25ff-d12d-40ea-822b-7f0fabf1569e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print keys\n",
    "print(\"Keys:\", list(result.keys()))\n",
    "\n",
    "# Print values\n",
    "#print(\"Values:\", list(result.values()))\n",
    "\n",
    "# Print key-value pairs (items)\n",
    "#print(\"Items:\", list(result.items()))\n",
    "\n",
    "#dgms_value = result['dgms']\n",
    "#print(\"Contents of 'dgms':\", dgms_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2271f20-c86f-4588-a2d3-2eb5e8a7da04",
   "metadata": {},
   "source": [
    "### Extract Betti numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a6492b-e711-471e-8ba5-97e933c892b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'dgms' in result:\n",
    "    persistence_diagrams = result['dgms']\n",
    "\n",
    "    # Extract Betti-0 numbers\n",
    "    betti_0_numbers = sum(np.isfinite(persistence_diagrams[0][:, 1]))\n",
    "    print(\"Betti-0 Numbers:\", betti_0_numbers)\n",
    "\n",
    "    # Extract Betti-1 numbers\n",
    "    betti_1_numbers = sum(np.isfinite(persistence_diagrams[1][:, 1]))\n",
    "    print(\"Betti-1 Numbers:\", betti_1_numbers)\n",
    "\n",
    "    # Extract Betti-2 numbers\n",
    "    betti_2_numbers = sum(np.isfinite(persistence_diagrams[2][:, 1]))\n",
    "    print(\"Betti-2 Numbers:\", betti_2_numbers)\n",
    "\n",
    "else:\n",
    "    print(\"The 'dgms' key is not present in the dictionary.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d605e7-b0f6-4362-9a31-554f446172a8",
   "metadata": {},
   "source": [
    "### View patient record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b474bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View result dictionary for patient record\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28940ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r'tpehg914'\n",
    "\n",
    "# Read the WFDB record\n",
    "record_inspect = wfdb.rdrecord(file_path)\n",
    "\n",
    "# Print the record object\n",
    "print(record_inspect)\n",
    "\n",
    "# Access the raw signal data\n",
    "raw_data = record_inspect.p_signal\n",
    "print(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10125d4-8457-445e-b1df-2057ef546c3b",
   "metadata": {},
   "source": [
    "## Compile filtration results for all observations - unfiltered sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743fefc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop to compile results list for multiple records\n",
    "\n",
    "# Set the folder path where CSV files are located\n",
    "folder_path = r'/home/katie_grillaert/ehg_preterm/tpehgdb'\n",
    "\n",
    "# Set the sample fraction\n",
    "sample_fraction = 0.01\n",
    "\n",
    "# Initialize a counter variable\n",
    "counter = 0\n",
    "\n",
    "# Get a list of all CSV files in the folder\n",
    "csv_files = glob.glob(folder_path + '/*.csv')\n",
    "\n",
    "# Dictionary to store results with filenames as keys\n",
    "all_results_unfilt = {}\n",
    "\n",
    "# Loop through each CSV file\n",
    "for csv_file in csv_files:\n",
    "\n",
    "    # Increment the counter\n",
    "    counter += 1\n",
    "\n",
    "    # Print the file number and total number of files\n",
    "    print(f\"Processing file {counter} of {len(csv_files)}\")\n",
    "    \n",
    "    # Extract the filename without the path\n",
    "    filename = os.path.basename(csv_file)\n",
    "   \n",
    "    # Extract the desired part of the filename, e.g., without the extension\n",
    "    key = filename.replace('.csv', '')\n",
    "\n",
    "    # Read CSV file into a DataFrame\n",
    "    df_filename = pd.read_csv(csv_file, header=None)\n",
    "    \n",
    "    # Increment the counter\n",
    "    counter += 1\n",
    "\n",
    "    # Print the file number and total number of files\n",
    "    print(f\"Processing file {counter} of {len(csv_files)}\")    \n",
    "    \n",
    "    # Select only numeric columns\n",
    "    numeric_columns = df_filename.select_dtypes(include=['number'])\n",
    "\n",
    "    # Check if there is at least one numeric column\n",
    "    if not numeric_columns.empty:\n",
    "\n",
    "        # Rename cols\n",
    "        numeric_columns.columns = numeric_columns.columns.astype(str)\n",
    "                \n",
    "        new_column_names = {'0': 's1_unfilt', \n",
    "                         '1': 's1_filter_a', \n",
    "                         '2': 's1_filter_b',\n",
    "                         '3': 's1_filter_c',\n",
    "                         '4': 's2_unfilt', \n",
    "                         '5': 's2_filter_a', \n",
    "                         '6': 's2_filter_b',\n",
    "                         '7': 's2_filter_c',\n",
    "                         '8': 's3_unfilt', \n",
    "                         '9': 's3_filter_a', \n",
    "                         '10': 's3_filter_b',\n",
    "                         '11': 's3_filter_c'}\n",
    "\n",
    "        numeric_columns.rename(columns=new_column_names, inplace=True)\n",
    "\n",
    "        # Select only columns for sensor groups\n",
    "        unfilt_group = ['s1_unfilt', 's2_unfilt', 's3_unfilt']\n",
    "        #filt_a_group = ['s1_filter_a', 's2_filter_a', 's3_filter_a']\n",
    "        #filt_b_group = ['s1_filter_b', 's2_filter_b', 's3_filter_b']\n",
    "        #filt_c_group = ['s1_filter_c', 's2_filter_c', 's3_filter_c']\n",
    "\n",
    "        # Create df subsets\n",
    "        unfilt_df = numeric_columns[unfilt_group]\n",
    "        #filt_a_df = numeric_columns[filt_a_group]\n",
    "        #filt_b_df = numeric_columns[filt_b_group]\n",
    "        #filt_c_df = numeric_columns[filt_c_group]\n",
    " \n",
    "        # Take a sample of the DataFrame\n",
    "        df_filename_sample = unfilt_df.sample(frac=sample_fraction, random_state=42)\n",
    "\n",
    "        # Print debugging information\n",
    "        #print(f\"Sampled DataFrame: {df_filename_sample}\")\n",
    "\n",
    "        # Perform persistent homology analysis\n",
    "        result_filename = ripser(df_filename_sample, maxdim=2, coeff=2, metric='euclidean')\n",
    "\n",
    "        # Add the 'filename' key to the result dictionary\n",
    "        result_filename['filename'] = key\n",
    "\n",
    "        # Store the result in the dictionary with the modified filename as the key\n",
    "        all_results_unfilt[key] = result_filename\n",
    "        \n",
    "    else:\n",
    "        print(\"No numeric columns found in the DataFrame.\")\n",
    "\n",
    "\n",
    "# Now, all_results contains the results for each patient with filenames as keys\n",
    "print(all_results_unfilt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec4e407-17b6-4dbc-852d-af96b1a0d157",
   "metadata": {},
   "source": [
    "### Inspect dictionary from all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8166c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in all_results_unfilt:\n",
    "    print(key)\n",
    "\n",
    "#print(\"Keys:\", list(all_results.keys()))\n",
    "#dgms_for_specific_file = all_results['tpehg1484']['dgms']\n",
    "#dgms_for_specific_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a248a953-d778-4c30-b038-31c2ab370b6b",
   "metadata": {},
   "source": [
    "## Extract all Betti numbers and filenames to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3764c49-8523-4500-a701-137c6bcd074d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists to store betti numbers and filenames\n",
    "betti_numbers_list = []\n",
    "filenames_list = []\n",
    "\n",
    "for filename, result in all_results_unfilt.items():\n",
    "    # Check if 'dgms' key is present\n",
    "    if 'dgms' in result:\n",
    "        persistence_diagrams = result['dgms']\n",
    "\n",
    "        # Extract Betti-0 numbers\n",
    "        betti_0_numbers = sum(np.isfinite(persistence_diagrams[0][:, 1]))\n",
    "\n",
    "        # Extract Betti-1 numbers\n",
    "        betti_1_numbers = sum(np.isfinite(persistence_diagrams[1][:, 1]))\n",
    "\n",
    "        # Extract Betti-2 numbers\n",
    "        betti_2_numbers = sum(np.isfinite(persistence_diagrams[2][:, 1]))\n",
    "\n",
    "        # Extract the filename without the extension\n",
    "        filename = result.get('filename', 'unknown')\n",
    "\n",
    "        # Append betti numbers and filename to the lists\n",
    "        betti_numbers_list.append([betti_0_numbers, betti_1_numbers, betti_2_numbers])\n",
    "        filenames_list.append(filename)\n",
    "\n",
    "    else:\n",
    "        print(\"The 'dgms' key is not present in the dictionary.\")\n",
    "        \n",
    "# Create a DataFrame from the lists\n",
    "df_betti_numbers = pd.DataFrame(betti_numbers_list, columns=['Betti-0', 'Betti-1', 'Betti-2'])\n",
    "\n",
    "# Add the 'filename' column to the DataFrame\n",
    "df_betti_numbers['filename'] = filenames_list\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_betti_numbers.to_csv('/home/katie_grillaert/ehg_preterm/betti_numbers_unfilt.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad3b56c-8f5d-46e7-b9de-1040f6657e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "betti_numbers_df = pd.read_csv('/home/katie_grillaert/ehg_preterm/betti_numbers_unfilt.csv')\n",
    "betti_numbers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb5413c-c514-4220-b184-2a15effe80e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for filtered sensor groups\n",
    "# Loop to compile results list for multiple records\n",
    "\n",
    "#subset = [unfilt_df, filt_a_df, filt_b_df, filt_c_df]\n",
    "\n",
    "for i in range(4):\n",
    "    \n",
    "    subset = f\"Subset_{i}\"\n",
    "\n",
    "    # Set the folder path where CSV files are located\n",
    "    folder_path = r'/home/katie_grillaert/ehg_preterm/tpehgdb'\n",
    "\n",
    "    # Set the sample fraction\n",
    "    sample_fraction = 0.01\n",
    "\n",
    "    # Initialize a counter variable\n",
    "    counter = 0\n",
    "    \n",
    "    # Dictionary to store results with filenames as keys\n",
    "    all_results = {}\n",
    "    \n",
    "    # Get a list of all CSV files in the folder\n",
    "    csv_files = glob.glob(folder_path + '/*.csv')\n",
    "\n",
    "    # Loop through each CSV file\n",
    "    for csv_file in csv_files:\n",
    "\n",
    "        # Increment the counter\n",
    "        counter += 1\n",
    "    \n",
    "        # Extract the filename without the path\n",
    "        filename = os.path.basename(csv_file)\n",
    "   \n",
    "        # Extract the desired part of the filename, e.g., without the extension\n",
    "        key = filename.replace('.csv', '')\n",
    "\n",
    "        # Read CSV file into a DataFrame\n",
    "        df_filename = pd.read_csv(csv_file, header=None)\n",
    "\n",
    "        # Print the file number and total number of files\n",
    "        print(f\"Processing file {counter} of {len(csv_files)}\", filename)    \n",
    "    \n",
    "        # Select only numeric columns\n",
    "        numeric_columns = df_filename.select_dtypes(include=['number'])\n",
    "\n",
    "        # Check if there is at least one numeric column\n",
    "        if not numeric_columns.empty:\n",
    "\n",
    "            # Rename cols\n",
    "            numeric_columns.columns = numeric_columns.columns.astype(str)\n",
    "                \n",
    "            new_column_names = {'0': 's1_unfilt', \n",
    "                             '1': 's1_filter_a', \n",
    "                             '2': 's1_filter_b',\n",
    "                             '3': 's1_filter_c',\n",
    "                             '4': 's2_unfilt', \n",
    "                             '5': 's2_filter_a', \n",
    "                             '6': 's2_filter_b',\n",
    "                             '7': 's2_filter_c',\n",
    "                             '8': 's3_unfilt', \n",
    "                             '9': 's3_filter_a', \n",
    "                             '10': 's3_filter_b',\n",
    "                             '11': 's3_filter_c'}\n",
    "\n",
    "            numeric_columns.rename(columns=new_column_names, inplace=True)\n",
    "\n",
    "            # Select only columns for sensor groups\n",
    "            unfilt_group = ['s1_unfilt', 's2_unfilt', 's3_unfilt']\n",
    "            filt_a_group = ['s1_filter_a', 's2_filter_a', 's3_filter_a']\n",
    "            filt_b_group = ['s1_filter_b', 's2_filter_b', 's3_filter_b']\n",
    "            filt_c_group = ['s1_filter_c', 's2_filter_c', 's3_filter_c']\n",
    "\n",
    "            # Create df subsets\n",
    "            unfilt_df = numeric_columns[unfilt_group]\n",
    "            filt_a_df = numeric_columns[filt_a_group]\n",
    "            filt_b_df = numeric_columns[filt_b_group]\n",
    "            filt_c_df = numeric_columns[filt_c_group]\n",
    "\n",
    "            # Choose the appropriate subset dataframe based on the current iteration\n",
    "            if subset == \"Subset_0\":\n",
    "                subset_df = unfilt_df\n",
    "            elif subset == \"Subset_1\":\n",
    "                 subset_df = filt_a_df\n",
    "            elif subset == \"Subset_2\":\n",
    "                subset_df = filt_b_df\n",
    "            elif subset == \"Subset_3\":\n",
    "                subset_df = filt_c_df\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown subset: {subset}\")\n",
    "                \n",
    "            # Take a sample of the DataFrame\n",
    "            df_filename_sample = subset_df.sample(frac=sample_fraction, random_state=42)\n",
    "\n",
    "            # Print debugging information\n",
    "            #print(f\"Sampled DataFrame: {df_filename_sample}\")\n",
    "\n",
    "            # Perform persistent homology analysis\n",
    "            result_filename = ripser(df_filename_sample, maxdim=2, coeff=2, metric='euclidean')\n",
    "    \n",
    "            # Add the 'filename' key to the result dictionary\n",
    "            result_filename['filename'] = key\n",
    "\n",
    "            # Store the result in the dictionary with the modified filename as the key\n",
    "            all_results[key] = result_filename\n",
    "        \n",
    "        else:\n",
    "            print(\"No numeric columns found in the DataFrame.\")\n",
    "    \n",
    "    # Now, all_results contains the results for each patient with filenames as keys\n",
    "    print(\"Subset\", i, \"completed\")\n",
    "\n",
    "    # Create lists to store betti numbers and filenames\n",
    "    betti_numbers_list = []\n",
    "    filenames_list = []\n",
    "\n",
    "    for filename, result in all_results.items():\n",
    "        # Check if 'dgms' key is present\n",
    "        if 'dgms' in result:\n",
    "            persistence_diagrams = result['dgms']\n",
    "\n",
    "            # Extract Betti-0 numbers\n",
    "            betti_0_numbers = sum(np.isfinite(persistence_diagrams[0][:, 1]))\n",
    "\n",
    "            # Extract Betti-1 numbers\n",
    "            betti_1_numbers = sum(np.isfinite(persistence_diagrams[1][:, 1]))\n",
    "\n",
    "            # Extract Betti-2 numbers\n",
    "            betti_2_numbers = sum(np.isfinite(persistence_diagrams[2][:, 1]))\n",
    "\n",
    "            # Extract the filename without the extension\n",
    "            filename = result.get('filename', 'unknown')\n",
    "\n",
    "            # Append betti numbers and filename to the lists\n",
    "            betti_numbers_list.append([betti_0_numbers, betti_1_numbers, betti_2_numbers])\n",
    "            filenames_list.append(filename)\n",
    "\n",
    "        else:\n",
    "            print(\"The 'dgms' key is not present in the dictionary.\")\n",
    "        \n",
    "    # Create a DataFrame from the lists\n",
    "    df_betti_numbers = pd.DataFrame(betti_numbers_list, columns=[f'Betti-0_{subset}', f'Betti_1_{subset}', f'Betti_2_{subset}'])\n",
    "\n",
    "\n",
    "    # Add the 'filename' column to the DataFrame\n",
    "    df_betti_numbers['filename'] = filenames_list\n",
    "\n",
    "    # Generate the output CSV filename dynamically based on the current subset\n",
    "    output_csv_filename = f'/home/katie_grillaert/ehg_preterm/betti_numbers_{subset}.csv'\n",
    "\n",
    "    # Save the DataFrame to the CSV file\n",
    "    df_betti_numbers.to_csv(output_csv_filename, index=False)\n",
    "\n",
    "    print(f\"Results saved to: {output_csv_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507e521c-fe04-47ca-a13b-ecd711e196ed",
   "metadata": {},
   "source": [
    "## Examine Header Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a0d0db-8892-4314-92e0-ba61e7723f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "hea_file_path = '/home/katie_grillaert/ehg_preterm/tpehgdb/tpehg1484'\n",
    "\n",
    "# Load the HEA file\n",
    "hea_record = wfdb.rdheader(hea_file_path)\n",
    "\n",
    "# Display general information about the record\n",
    "print(f\"Record Name: {hea_record.record_name}\")\n",
    "print(f\"Number of Channels: {hea_record.n_sig}\")\n",
    "print(f\"Sampling Frequency: {hea_record.fs} Hz\")\n",
    "print(f\"Signal Labels: {hea_record.sig_name}\")\n",
    "print(f\"Comments: {hea_record.comments}\")\n",
    "\n",
    "# Display detailed information about each signal/channel\n",
    "#for i, signal in enumerate(hea_record.sig_name):\n",
    " #   print(f\"\\nSignal {i + 1} - {signal}\")\n",
    "  #  print(f\"Label: {hea_record.sig_name[i]}\")\n",
    "   # print(f\"Units: {hea_record.units[i]}\")\n",
    "   # print(f\"ADC Gain: {hea_record.adc_gain[i]}\")\n",
    "   # print(f\"ADC Resolution: {hea_record.adc_res[i]}\")\n",
    "   # print(f\"Baseline: {hea_record.baseline[i]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a24ef7c-522f-41cd-880f-c000050bf0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the HEA file\n",
    "hea_file_path = '/home/katie_grillaert/ehg_preterm/tpehgdb/tpehg1484'\n",
    "hea_record = wfdb.rdheader(hea_file_path)\n",
    "\n",
    "# Extract comments from the record\n",
    "comments = hea_record.comments\n",
    "\n",
    "# Create a DataFrame with comments as columns\n",
    "df = pd.DataFrame({'Comment{}'.format(i + 1): [comment] for i, comment in enumerate(comments)})\n",
    "\n",
    "# Specify the desired CSV file path\n",
    "csv_file_path = '/home/katie_grillaert/ehg_preterm/tpehg1484_comments.csv'\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(f\"Comments have been saved to {csv_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924b891d-79e1-415b-9fac-81b00f0a570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_header = pd.read_csv(\"/home/katie_grillaert/ehg_preterm/tpehg1484_comments.csv\")\n",
    "df_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186ecc68-3249-41e4-b124-99a70f67b7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import wfdb\n",
    "\n",
    "# Directory path containing HEA files\n",
    "hea_directory = '/home/katie_grillaert/ehg_preterm/tpehgdb/'\n",
    "\n",
    "# Use glob to get all HEA files in the directory\n",
    "hea_files = glob.glob(os.path.join(hea_directory, '*.hea'))\n",
    "\n",
    "# Initialize an empty list to store rows\n",
    "rows = []\n",
    "\n",
    "# Process each HEA file\n",
    "for hea_file_path in hea_files:\n",
    "    # Extract file name without extension\n",
    "    file_name = os.path.splitext(os.path.basename(hea_file_path))[0]\n",
    "\n",
    "    print(file_name)\n",
    "    # Check if the corresponding .hea file exists\n",
    "    if os.path.exists(hea_file_path):\n",
    "        # Load the HEA file\n",
    "        hea_record = wfdb.rdheader(file_name)\n",
    "\n",
    "        # Extract comments from the record\n",
    "        comments = hea_record.comments\n",
    "\n",
    "        # Create a dictionary representing a row\n",
    "        row_dict = {'filename': file_name}\n",
    "        for idx, comment in enumerate(comments):\n",
    "            row_dict[f'Comment{idx + 1}'] = comment\n",
    "\n",
    "        # Append the row dictionary to the list\n",
    "        rows.append(row_dict)\n",
    "\n",
    "# Create a DataFrame from the list of rows\n",
    "df_all_comments = pd.DataFrame(rows)\n",
    "\n",
    "# Specify the desired CSV file path\n",
    "csv_file_path = '/home/katie_grillaert/ehg_preterm/all_comments.csv'\n",
    "\n",
    "# Save the main DataFrame to CSV\n",
    "df_all_comments.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(f\"Comments for all HEA files have been saved to {csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1aa5ca9-8988-4733-8d8a-12f838194b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comments_df = pd.read_csv(\"/home/katie_grillaert/ehg_preterm/all_comments.csv\")\n",
    "all_comments_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c839fea-4564-44a1-8e78-27a6ed70548d",
   "metadata": {},
   "source": [
    "## Read in metadata from smr file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be23f82e-46a0-4824-abab-746e5722a71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/katie_grillaert/ehg_preterm/smr.txt'\n",
    "\n",
    "Read the content of the file\n",
    "with open(file_path, 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Use StringIO to convert the content to a file-like object\n",
    "data = StringIO(content)\n",
    "\n",
    "# Read the data into a DataFrame\n",
    "df = pd.read_csv(file_path, sep='|', skipinitialspace=True, skipfooter=1, engine='python', header = 0)\n",
    "\n",
    "smr_df.rename(columns={'Record    ': 'filename'}, inplace=True)\n",
    "smr_df.rename(columns={' Gestation ': 'Gestation'}, inplace=True)\n",
    "smr_df.rename(columns={' Rec. time ': 'Rec_time'}, inplace=True)\n",
    "smr_df.rename(columns={'   Group   ': 'Group'}, inplace=True)\n",
    "smr_df.rename(columns={' Premature ': 'Premature'}, inplace=True)\n",
    "smr_df.rename(columns={' Early ': 'Early'}, inplace=True)\n",
    "\n",
    "\n",
    "smr_df.to_csv('/home/katie_grillaert/ehg_preterm/smr.csv', index=False)\n",
    "smr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc16effc-0bcd-4eed-9541-9e08fafc8f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#smr_df.columns\n",
    "\n",
    "# Remove leading and trailing spaces from all entries in the DataFrame\n",
    "#smr_df = smr_df.map(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "#smr_df\n",
    "#smr_df.to_csv('/home/katie_grillaert/ehg_preterm/smr.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22860920-94a6-43fb-b79d-938529fb623b",
   "metadata": {},
   "source": [
    "## Merge Betti numbers, smr metadata, and Comments on filename "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f08974-ddce-4d8b-affd-30d7eade0591",
   "metadata": {},
   "outputs": [],
   "source": [
    "betti_0 = pd.read_csv(\"/home/katie_grillaert/ehg_preterm/betti_numbers_Subset_0.csv\")\n",
    "betti_1 = pd.read_csv(\"/home/katie_grillaert/ehg_preterm/betti_numbers_Subset_1.csv\")\n",
    "betti_2 = pd.read_csv(\"/home/katie_grillaert/ehg_preterm/betti_numbers_Subset_2.csv\")\n",
    "#betti_3 = pd.read_csv(\"/home/katie_grillaert/ehg_preterm/betti_numbers_Subset_3.csv\")\n",
    "smr = pd.read_csv(\"/home/katie_grillaert/ehg_preterm/smr.csv\")\n",
    "comments = pd.read_csv(\"/home/katie_grillaert/ehg_preterm/all_comments.csv\")\n",
    "\n",
    "# Merge betti DataFrames one by one\n",
    "merged_df = pd.merge(betti_0, betti_1, on='filename', how='inner')\n",
    "merged_df = pd.merge(merged_df, betti_2, on='filename', how='inner')\n",
    "#merged_df = pd.merge(merged_df, betti_3, on='filename', how='inner')\n",
    "\n",
    "# Merge with smr DataFrame\n",
    "merged_df = pd.merge(merged_df, smr, on='filename', how='inner')\n",
    "\n",
    "# Merge with comments DataFrame\n",
    "merged_df = pd.merge(merged_df, comments, on='filename', how='inner')\n",
    "\n",
    "ehgdf = merged_df.copy()\n",
    "ehgdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162da86a-6e04-4ef0-be6d-f6dcff5d199b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ehgdf.to_csv('ehgdf.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7422f3e0-3fb9-4342-bdc4-fb200eb7012d",
   "metadata": {},
   "source": [
    "## Rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6695f3-6ec8-4852-883d-c43fe144ce73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to map old column names to new column names\n",
    "#cols_to_drop=[\"Comment1\", \"Comment2\"]\n",
    "#ehgdf.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "#column_order = [\"filename\"] + [col for col in ehgdf.columns if col != target_column]\n",
    "#ehgdf = ehgdf[column_order]\n",
    "\n",
    "\n",
    "column_mapping = {\n",
    "    'Comment3': 'Gestation',\n",
    "    'Comment4': 'Rec_Time',\n",
    "    'Comment5': 'Age',\n",
    "    'Comment6': 'Parity',\n",
    "    'Comment7': 'Abortions',\n",
    "    'Comment8': 'Weight',\n",
    "    'Comment9': 'Hypertension',\n",
    "    'Comment10': 'Diabetes',\n",
    "    'Comment11': 'Placental_pos',\n",
    "    'Comment12': 'Bleeding_first_tri',\n",
    "    'Comment13': 'Bleeding_second_tri',\n",
    "    'Comment14': 'Funneling',\n",
    "    'Comment15': 'Smoker',\n",
    "}\n",
    "\n",
    "# Rename columns using the rename method\n",
    "ehgdf.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "# Display the DataFrame with renamed columns\n",
    "ehgdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eede51f9-011b-4004-a4da-124dda822d43",
   "metadata": {},
   "source": [
    "## Check column datatypes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5d81cd-9f5c-4e41-aafc-96d965f096f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in ehgdf.columns:\n",
    "    data_type = ehgdf[column].dtype\n",
    "    print(f\"Column '{column}' has datatype: {data_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e370d50a-e500-4f1a-9c3f-7fdad6c2b043",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ehgdf.columns\n",
    "\n",
    "for column in columns:\n",
    "    unique_entries = ehgdf[column].unique()\n",
    "    print(\"Unique entries in\", column)\n",
    "    print(unique_entries)\n",
    "\n",
    "    value_counts = ehgdf[column].value_counts()\n",
    "    print(\"Count of each unique entry in\", column)\n",
    "    print(value_counts)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8c9865-6579-4b74-972e-08427cf42263",
   "metadata": {},
   "source": [
    "## Change dataypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fc5235-cd10-45cc-b869-dd59175986cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Floats\n",
    "ehgdf['Gestation'] = ehgdf['Gestation'].astype(float)\n",
    "ehgdf['Rec_time'] = ehgdf['Rec_time'].astype(float)\n",
    "ehgdf['Age'] = ehgdf['Age'].astype(float)\n",
    "ehgdf['Parity'] = ehgdf['Parity'].astype(float)\n",
    "ehgdf['Abortions'] = ehgdf['Abortions'].astype(float)\n",
    "ehgdf['Weight'] = ehgdf['Weight'].astype(float)\n",
    "\n",
    "#Booleans\n",
    "#ehgdf['Smoker'] = ehgdf['Smoker'].map({'Yes': True, 'No': False}).astype(bool)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a5c4ea-cf60-433f-8eb9-d3caebd2d49d",
   "metadata": {},
   "source": [
    "## Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4388ee88-6c2a-455c-b398-dc6c78e280ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a profile report\n",
    "profile = ProfileReport(ehgdf, title='EHG Data Profiling Report', explorative=True)\n",
    "\n",
    "# Save the report to an HTML file\n",
    "profile.to_file(\"ehg_output_report.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e327f91-5184-4b97-807d-61533d66019d",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "\n",
    "Run Loop on Filtrations\n",
    "\n",
    "Data cleaning and visualization\n",
    "- assign datatypes\n",
    "- clean up cells\n",
    "\n",
    "ML Pipeline\n",
    "- train/test split\n",
    "- select models to train on data\n",
    "- cross-validation\n",
    "- hyperparameter tuning\n",
    "- test\n",
    "\n",
    "To Research\n",
    "- use filtered data?\n",
    "- how to calc sparse matrix\n",
    "- is it ok to sample 1%?\n",
    "- what coeff?\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0e3c63-a856-4850-9e60-d3faeb571410",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
